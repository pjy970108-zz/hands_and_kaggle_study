{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week_36_LGB and FM [18th Place - 0.40604].ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZeYDvBVNCGB",
        "outputId": "462d02a5-1e80-48b2-a4fc-001e4c7198c1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZ8Wt8NOOFDj",
        "outputId": "c5928c9b-fc83-4e09-d4c5-e64af7c98770"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unnZ2MVNNrQB",
        "outputId": "9bed2ff8-85a5-4d57-ef4e-b0d7ec7a91a8"
      },
      "source": [
        "!pip install Wordbatch"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Wordbatch\n",
            "  Downloading Wordbatch-1.4.8.tar.gz (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from Wordbatch) (0.29.24)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from Wordbatch) (0.22.2.post1)\n",
            "Collecting python-Levenshtein\n",
            "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting py-lz4framed\n",
            "  Downloading py-lz4framed-0.14.0.tar.gz (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 57.9 MB/s \n",
            "\u001b[?25hCollecting randomgen==1.16.6\n",
            "  Downloading randomgen-1.16.6-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 36.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from Wordbatch) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from Wordbatch) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from Wordbatch) (1.1.5)\n",
            "Requirement already satisfied: wheel>=0.33.4 in /usr/local/lib/python3.7/dist-packages (from Wordbatch) (0.37.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from randomgen==1.16.6->Wordbatch) (57.4.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->Wordbatch) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->Wordbatch) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->Wordbatch) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->Wordbatch) (1.1.0)\n",
            "Building wheels for collected packages: Wordbatch, py-lz4framed, python-Levenshtein\n",
            "  Building wheel for Wordbatch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Wordbatch: filename=Wordbatch-1.4.8-cp37-cp37m-linux_x86_64.whl size=2742092 sha256=d712076e98e004d0ad8e2eca8236ea3728a99d1aa2f1f777b752a8de7ec1885a\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/cd/90/66c3ba883859da7c9d5ab57b301820c414c8430134c9ec8621\n",
            "  Building wheel for py-lz4framed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-lz4framed: filename=py_lz4framed-0.14.0-cp37-cp37m-linux_x86_64.whl size=346649 sha256=24dc4482a9912f43f784fa73a87425c65c8614e3f09abf5123549c72dfa4630d\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/9c/8e/5d008dfcbb83cfb99763f100d10b6b2d953274f48744b7be81\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp37-cp37m-linux_x86_64.whl size=149856 sha256=97509645b50c7d7ae58c22576040945443bb601824a79c25706ef8f94ea5cc5e\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/5f/ca/7c4367734892581bb5ff896f15027a932c551080b2abd3e00d\n",
            "Successfully built Wordbatch py-lz4framed python-Levenshtein\n",
            "Installing collected packages: randomgen, python-Levenshtein, py-lz4framed, Wordbatch\n",
            "Successfully installed Wordbatch-1.4.8 py-lz4framed-0.14.0 python-Levenshtein-0.12.2 randomgen-1.16.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2_au3FButeH4",
        "outputId": "b32611f2-266a-4ee6-fd15-f82c4b36da85"
      },
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "SUBMIT_MODE = True\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import gc\n",
        "import string\n",
        "import re\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from scipy.sparse import csr_matrix, hstack\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection.univariate_selection import SelectKBest, f_regression\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "import wordbatch\n",
        "from wordbatch.extractors import WordBag\n",
        "from wordbatch.models import FM_FTRL\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import lightgbm as lgb\n",
        "\n",
        "# 평가지표\n",
        "def rmse(predicted, actual):\n",
        "    return np.sqrt(((predicted - actual) ** 2).mean())\n",
        "# text split하는것\n",
        "def split_cat(text):\n",
        "  try:\n",
        "    return text.split(\"/\")\n",
        "  except:\n",
        "    return (\"No Label\", \"No Label\", \"No Label\")\n",
        "class TargetEncoder:\n",
        "    def __repr__(self):\n",
        "        return 'TargetEncoder'\n",
        "    # 파라메타\n",
        "    def __init__(self, cols, smoothing=1, min_samples_leaf=1, noise_level=0, keep_original=False):\n",
        "        self.cols = cols\n",
        "        self.smoothing = smoothing\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.noise_level = noise_level\n",
        "        self.keep_original = keep_original\n",
        "\n",
        "    @staticmethod\n",
        "    def add_noise(series, noise_level): \n",
        "        return series * (1 + noise_level * np.random.randn(len(series)))\n",
        "\n",
        "    def encode(self, train, test, target):  # self keep original이면 encode columns에 te붙임\n",
        "        for col in self.cols: \n",
        "            if self.keep_original:\n",
        "                train[col + '_te'], test[col + '_te'] = self.encode_column(train[col], test[col], target)\n",
        "            else:\n",
        "                train[col], test[col] = self.encode_column(train[col], test[col], target)\n",
        "        return train, test\n",
        "\n",
        "    def encode_column(self, trn_series, tst_series, target):\n",
        "        temp = pd.concat([trn_series, target], axis=1)\n",
        "        # target 평균 계산\n",
        "        averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
        "        # smoothing 계산\n",
        "        smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - self.min_samples_leaf) / self.smoothing))\n",
        "        prior = target.mean()\n",
        "        \n",
        "        # count가 클수록 avg가 덜 고려된다.        \n",
        "        averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
        "        averages.drop(['mean', 'count'], axis=1, inplace=True)\n",
        "\n",
        "        # trn과 tst 시리즈의 평균 통합하고 결측치 prior로 대체\n",
        "\n",
        "        ft_trn_series = pd.merge(\n",
        "            trn_series.to_frame(trn_series.name),\n",
        "            averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
        "            on=trn_series.name,\n",
        "            how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
        "        ft_trn_series.index = trn_series.index\n",
        "\n",
        "        ft_tst_series = pd.merge(\n",
        "            tst_series.to_frame(tst_series.name),\n",
        "            averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
        "            on=tst_series.name,\n",
        "            how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
        "        ft_tst_series.index = tst_series.index\n",
        "        # add noise 한 test, train 값 추출\n",
        "        return self.add_noise(ft_trn_series, self.noise_level), self.add_noise(ft_tst_series, self.noise_level)   \n",
        "\n",
        "def to_number(x):\n",
        "    try:\n",
        "        if not x.isdigit():\n",
        "            return 0\n",
        "        x = int(x)\n",
        "        if x > 100:\n",
        "            return 100\n",
        "        else:\n",
        "            return x\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "def sum_numbers(desc):\n",
        "    if not isinstance(desc, str):\n",
        "        return 0\n",
        "    try:\n",
        "        return sum([to_number(s) for s in desc.split()])\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "\n",
        "\n",
        "  # stopword 제거 및 text normalization 진행\n",
        "stopwords = {x: 1 for x in stopwords.words('english')}\n",
        "non_alphanums = re.compile(u'[^A-Za-z0-9]+')\n",
        "non_alphanumpunct = re.compile(u'[^A-Za-z0-9\\.?!,; \\(\\)\\[\\]\\'\\\"\\$]+')\n",
        "RE_PUNCTUATION = '|'.join([re.escape(x) for x in string.punctuation])  \n",
        "\n",
        "  #\" \" 붙이기\n",
        "def normalize_text(text):\n",
        "    return u\" \".join(\n",
        "        [x for x in [y for y in non_alphanums.sub(' ', text).lower().strip().split(\" \")] \\\n",
        "        if len(x) > 1 and x not in stopwords])\n",
        "\n",
        "def clean_name(x):\n",
        "    if len(x):\n",
        "        x = non_alphanums.sub(' ', x).split()\n",
        "        if len(x):\n",
        "            return x[0].lower()\n",
        "    return ''\n",
        "\n",
        "\n",
        "print('[{}] Finished defining stuff'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "train = pd.read_table(\"/content/drive/MyDrive/kaggle_code/week_34/train.tsv\", engine='c', \n",
        "                      dtype={'item_condition_id': 'category',\n",
        "                            'shipping': 'category',\n",
        "                            }, \n",
        "                    converters={'category_name': split_cat})\n",
        "test = pd.read_table(\"/content/drive/MyDrive/kaggle_code/week_34/test.tsv\", engine='c', \n",
        "                      dtype={'item_condition_id': 'category',\n",
        "                            'shipping': 'category',\n",
        "                            },\n",
        "                    converters={'category_name': split_cat})\n",
        "\n",
        "print('[{}] Finished load data'.format(time.time() - start_time))\n",
        "train['is_train']= 1\n",
        "test['is_train'] = 0\n",
        "print('[{}] compiled train/test'.format(time.time()- start_time))\n",
        "print('Train shape: ', train.shape)\n",
        "print('Test shape: ', test.shape)\n",
        "# price가 0이 아닌거만 \n",
        "train = train[train.price != 0].reset_index(drop=True)\n",
        "print('[{}] Removed nonzero price'.format(time.time() - start_time))\n",
        "print('Train shape: ', train.shape)\n",
        "print('Test shape: ', test.shape)  \n",
        "\n",
        "\n",
        "# y값 로그 처리\n",
        "y = np.log1p(train['price'])\n",
        "nrow_train = train.shape[0]\n",
        "print(test.columns)\n",
        "merge = pd.concat([train, test])\n",
        "submission = test[['test_id']]\n",
        "print('[{}] Compiled merge'.format(time.time() - start_time))\n",
        "print('Merge shape: ', merge.shape)\n",
        "\n",
        "\n",
        "del train\n",
        "del test\n",
        "\n",
        "merge.drop(['train_id', 'test_id', 'price'], axis=1, inplace=True)\n",
        "gc.collect()\n",
        "print('[{}] Garbage collection'.format(time.time() - start_time))\n",
        "# 결측치 처리\n",
        "\n",
        "# str로 분할된 list 중 0번째를 추출하고 결측치를 missing처리\n",
        "merge['gencat_name'] = merge['category_name'].str.get(0).replace('', 'missing').astype('category')\n",
        "merge['subcat1_name'] = merge['category_name'].str.get(1).fillna('missing').astype('category')\n",
        "merge['subcat2_name'] = merge['category_name'].str.get(2).fillna('missing').astype('category')\n",
        "merge.drop('category_name', axis=1, inplace=True)\n",
        "print('[{}] Split categories completed.'.format(time.time() - start_time))\n",
        "\n",
        "# 아이템 shipping 등의 결측치 처리\n",
        "merge['item_condition_id'] = merge['item_condition_id'].cat.add_categories(['missing']).fillna('missing')\n",
        "merge['shipping'] = merge['shipping'].cat.add_categories(['missing']).fillna('missing')\n",
        "merge['item_description'].fillna('missing', inplace=True)\n",
        "merge['brand_name'] = merge['brand_name'].fillna('missing').astype('category')\n",
        "print('[{}] Handle missing completed.'.format(time.time() - start_time))\n",
        "\n",
        "# columns 추가\n",
        "merge['name_first'] = merge['name'].apply(clean_name)\n",
        "print('[{}] FE 1/37'.format(time.time() - start_time))\n",
        "\n",
        "merge['name_first_count'] = merge.groupby('name_first')['name_first'].transform('count')\n",
        "print('[{}] FE 2/37'.format(time.time() - start_time))\n",
        "\n",
        "merge['gencat_name_count'] = merge.groupby('gencat_name')['gencat_name'].transform('count')\n",
        "print('[{}] FE 3/37'.format(time.time() - start_time))\n",
        "\n",
        "merge['subcat1_name_count'] = merge.groupby('subcat1_name')['subcat1_name'].transform('count')\n",
        "print('[{}] FE 4/37'.format(time.time() - start_time))\n",
        "\n",
        "merge['subcat2_name_count'] = merge.groupby('subcat2_name')['subcat2_name'].transform('count')\n",
        "print('[{}] FE 5/37'.format(time.time() - start_time))\n",
        "\n",
        "merge['brand_name_count'] = merge.groupby('brand_name')['brand_name'].transform('count')\n",
        "print('[{}] FE 6/37'.format(time.time() - start_time))\n",
        "\n",
        "merge['NameLower'] = merge.name.str.count('[a-z]')\n",
        "print('[{}] FE 7/37'.format(time.time() - start_time))\n",
        "\n",
        "merge['DescriptionLower'] = merge.item_description.str.count('[a-z]')\n",
        "print('[{}] FE 8/37'.format(time.time() - start_time))\n",
        "\n",
        "merge['NameUpper'] = merge.name.str.count('[A-Z]')\n",
        "print('[{}] FE 9/37'.format(time.time() - start_time))\n",
        "\n",
        "merge['DescriptionUpper'] = merge.item_description.str.count('[A-Z]')\n",
        "print('[{}] FE 10/37'.format(time.time() - start_time))\n",
        "\n",
        "merge['name_len'] = merge['name'].apply(lambda x: len(x))\n",
        "print('[{}] FE 11/37'.format(time.time() - start_time))\n",
        "\n",
        "merge['des_len'] = merge['item_description'].apply(lambda x: len(x))\n",
        "print('[{}] FE 12/37'.format(time.time() - start_time))\n",
        "\n",
        "merge['name_desc_len_ratio'] = merge['name_len']/merge['des_len']\n",
        "print('[{}] FE 13/37'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "merge['desc_word_count'] = merge['item_description'].apply(lambda x: len(x.split()))\n",
        "print('[{}] FE 14/37'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "merge['mean_des'] = merge['item_description'].apply(lambda x: 0 if len(x) == 0 else float(len(x.split())) / len(x)) * 10\n",
        "print('[{}] FE 15/37'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "merge['name_word_count'] = merge['name'].apply(lambda x: len(x.split()))\n",
        "print('[{}] FE 16/37'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "merge['mean_name'] = merge['name'].apply(lambda x: 0 if len(x) == 0 else float(len(x.split())) / len(x))  * 10\n",
        "print('[{}] FE 17/37'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "merge['desc_letters_per_word'] = merge['des_len'] / merge['desc_word_count']\n",
        "print('[{}] FE 18/37'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "merge['name_letters_per_word'] = merge['name_len'] / merge['name_word_count']\n",
        "print('[{}] FE 19/37'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "merge['NameLowerRatio'] = merge['NameLower'] / merge['name_len']\n",
        "print('[{}] FE 20/37'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "merge['DescriptionLowerRatio'] = merge['DescriptionLower'] / merge['des_len']\n",
        "print('[{}] FE 21/37'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "merge['NameUpperRatio'] = merge['NameUpper'] / merge['name_len']\n",
        "print('[{}] FE 22/37'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "merge['DescriptionUpperRatio'] = merge['DescriptionUpper'] / merge['des_len']\n",
        "print('[{}] FE 23/37'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "merge['NamePunctCount'] = merge.name.str.count(RE_PUNCTUATION)\n",
        "print('[{}] FE 24/37'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "merge['DescriptionPunctCount'] = merge.item_description.str.count(RE_PUNCTUATION)\n",
        "print('[{}] FE 25/37'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "merge['NamePunctCountRatio'] = merge['NamePunctCount'] / merge['name_word_count']\n",
        "print('[{}] FE 26/37'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "merge['DescriptionPunctCountRatio'] = merge['DescriptionPunctCount'] / merge['desc_word_count']\n",
        "print('[{}] FE 27/37'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "merge['NameDigitCount'] = merge.name.str.count('[0-9]')\n",
        "print('[{}] FE 28/37'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "merge['DescriptionDigitCount'] = merge.item_description.str.count('[0-9]')\n",
        "print('[{}] FE 29/37'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "merge['NameDigitCountRatio'] = merge['NameDigitCount'] / merge['name_word_count']\n",
        "print('[{}] FE 30/37'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "merge['DescriptionDigitCountRatio'] = merge['DescriptionDigitCount']/merge['desc_word_count']\n",
        "print('[{}] FE 31/37'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "merge['stopword_ratio_desc'] = merge['item_description'].apply(lambda x: len([w for w in x.split() if w in stopwords])) / merge['desc_word_count']\n",
        "print('[{}] FE 32/37'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "merge['num_sum'] = merge['item_description'].apply(sum_numbers) \n",
        "print('[{}] FE 33/37'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "merge['weird_characters_desc'] = merge['item_description'].str.count(non_alphanumpunct)\n",
        "print('[{}] FE 34/37'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "merge['weird_characters_name'] = merge['name'].str.count(non_alphanumpunct)\n",
        "print('[{}] FE 35/37'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "merge['prices_count'] = merge['item_description'].str.count('[rm]')\n",
        "print('[{}] FE 36/37'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "merge['price_in_name'] = merge['item_description'].str.contains('[rm]', regex=False).astype('int')\n",
        "print('[{}] FE 37/37'.format(time.time() - start_time))\n",
        "\n",
        "# 핵심 컬럼 아닌 컬럼 분류???\n",
        "cols = set(merge.columns.values)\n",
        "basic_cols = {'name', 'item_condition_id', 'brand_name',\n",
        "  'shipping', 'item_description', 'gencat_name',\n",
        "  'subcat1_name', 'subcat2_name', 'name_first', 'is_train'}\n",
        "\n",
        "cols_to_normalize = cols - basic_cols - {'price_in_name'}\n",
        "other_cols = basic_cols | {'price_in_name'}\n",
        "\n",
        "merge_to_normalize = merge[list(cols_to_normalize)]\n",
        "merge_to_normalize = (merge_to_normalize - merge_to_normalize.mean()) / (merge_to_normalize.max() - merge_to_normalize.min())\n",
        "print('[{}] FE Normalized'.format(time.time() - start_time))\n",
        "\n",
        "merge = merge[list(other_cols)]\n",
        "merge = pd.concat([merge, merge_to_normalize],axis=1)\n",
        "print('[{}] FE Merged'.format(time.time() - start_time))\n",
        "\n",
        "del(merge_to_normalize)\n",
        "gc.collect()\n",
        "print('[{}] Garbage collection'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "# test train 분리\n",
        "df_test = merge.loc[merge['is_train'] == 0]\n",
        "df_train = merge.loc[merge['is_train'] == 1]\n",
        "del merge\n",
        "gc.collect()\n",
        "df_test = df_test.drop(['is_train'], axis=1)\n",
        "df_train = df_train.drop(['is_train'], axis=1)\n",
        "\n",
        "if SUBMIT_MODE:\n",
        "    y_train = y\n",
        "    del y\n",
        "    gc.collect()\n",
        "else:\n",
        "    df_train, df_test, y_train, y_test = train_test_split(df_train, y, test_size=0.2, random_state=144)\n",
        "\n",
        "print('[{}] Splitting completed.'.format(time.time() - start_time))  \n",
        "\n",
        "# 이게 어떤 모델인지 모르겠다. https://github.com/anttttti/Wordbatch\n",
        "wb = wordbatch.WordBatch(normalize_text, extractor=(WordBag, {\"hash_ngrams\": 2,\n",
        "                                                              \"hash_ngrams_weights\": [1.5, 1.0],\n",
        "                                                              \"hash_size\": 2 ** 29,\n",
        "                                                              \"norm\": None,\n",
        "                                                              \"tf\": 'binary',\n",
        "                                                              \"idf\": None,\n",
        "                                                              }), procs=8)\n",
        "wb.dictionary_freeze = True\n",
        "X_name_train = wb.fit_transform(df_train['name'])\n",
        "X_name_test = wb.transform(df_test['name'])\n",
        "del(wb)\n",
        "mask = np.where(X_name_train.getnnz(axis=0) > 3)[0]\n",
        "X_name_train = X_name_train[:, mask]\n",
        "X_name_test = X_name_test[:, mask]\n",
        "print('[{}] Vectorize `name` completed.'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "wb = wordbatch.WordBatch(normalize_text, extractor=(WordBag, {\"hash_ngrams\": 2,\n",
        "                                                              \"hash_ngrams_weights\": [1.0, 1.0],\n",
        "                                                              \"hash_size\": 2 ** 28,\n",
        "                                                              \"norm\": \"l2\",\n",
        "                                                              \"tf\": 1.0,\n",
        "                                                              \"idf\": None}), procs=8)\n",
        "wb.dictionary_freeze = True\n",
        "X_description_train = wb.fit_transform(df_train['item_description'])\n",
        "X_description_test = wb.transform(df_test['item_description'])\n",
        "del(wb)\n",
        "mask = np.where(X_description_train.getnnz(axis=0) > 3)[0]\n",
        "X_description_train = X_description_train[:, mask]\n",
        "X_description_test = X_description_test[:, mask]\n",
        "print('[{}] Vectorize `item_description` completed.'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "X_train_1, X_train_2, y_train_1, y_train_2 = train_test_split(X_description_train, y_train,\n",
        "                                                              test_size = 0.5,\n",
        "                                                              shuffle = False)\n",
        "print('[{}] Finished splitting'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "# Ridge 모델\n",
        "model = Ridge(solver=\"sag\", fit_intercept=True, random_state=205, alpha=3.3)\n",
        "model.fit(X_train_1, y_train_1)\n",
        "print('[{}] Finished to train desc ridge (1)'.format(time.time() - start_time))\n",
        "desc_ridge_preds1 = model.predict(X_train_2)\n",
        "desc_ridge_preds1f = model.predict(X_description_test)\n",
        "print('[{}] Finished to predict desc ridge (1)'.format(time.time() - start_time))\n",
        "\n",
        "model = Ridge(solver=\"sag\", fit_intercept=True, random_state=205, alpha=3.3)\n",
        "model.fit(X_train_2, y_train_2)\n",
        "print('[{}] Finished to train desc ridge (2)'.format(time.time() - start_time))\n",
        "desc_ridge_preds2 = model.predict(X_train_1)\n",
        "desc_ridge_preds2f = model.predict(X_description_test)\n",
        "print('[{}] Finished to predict desc ridge (2)'.format(time.time() - start_time))\n",
        "desc_ridge_preds_oof = np.concatenate((desc_ridge_preds2, desc_ridge_preds1), axis=0)\n",
        "desc_ridge_preds_test = (desc_ridge_preds1f + desc_ridge_preds2f) / 2.0\n",
        "print('RMSLE OOF: {}'.format(rmse(desc_ridge_preds_oof, y_train)))\n",
        "if not SUBMIT_MODE:\n",
        "    print('RMSLE TEST: {}'.format(rmse(desc_ridge_preds_test, y_test)))\n",
        "\n",
        "\n",
        "X_train_1, X_train_2, y_train_1, y_train_2 = train_test_split(X_name_train, y_train,\n",
        "                                                              test_size = 0.5,\n",
        "                                                              shuffle = False)\n",
        "print('[{}] Finished splitting'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "\n",
        "model = Ridge(solver=\"sag\", fit_intercept=True, random_state=205, alpha=3.3)\n",
        "model.fit(X_train_1, y_train_1)\n",
        "print('[{}] Finished to train name ridge (1)'.format(time.time() - start_time))\n",
        "name_ridge_preds1 = model.predict(X_train_2)\n",
        "name_ridge_preds1f = model.predict(X_name_test)\n",
        "print('[{}] Finished to predict name ridge (1)'.format(time.time() - start_time))\n",
        "model = Ridge(solver=\"sag\", fit_intercept=True, random_state=205, alpha=3.3)\n",
        "model.fit(X_train_2, y_train_2)\n",
        "print('[{}] Finished to train name ridge (2)'.format(time.time() - start_time))\n",
        "name_ridge_preds2 = model.predict(X_train_1)\n",
        "name_ridge_preds2f = model.predict(X_name_test)\n",
        "print('[{}] Finished to predict name ridge (2)'.format(time.time() - start_time))\n",
        "name_ridge_preds_oof = np.concatenate((name_ridge_preds2, name_ridge_preds1), axis=0)\n",
        "name_ridge_preds_test = (name_ridge_preds1f + name_ridge_preds2f) / 2.0\n",
        "print('RMSLE OOF: {}'.format(rmse(name_ridge_preds_oof, y_train)))\n",
        "if not SUBMIT_MODE:\n",
        "  print('RMSLE TEST: {}'.format(rmse(name_ridge_preds_test, y_test)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "del X_train_1\n",
        "del X_train_2\n",
        "del y_train_1\n",
        "del y_train_2\n",
        "del name_ridge_preds1\n",
        "del name_ridge_preds1f\n",
        "del name_ridge_preds2\n",
        "del name_ridge_preds2f\n",
        "del desc_ridge_preds1\n",
        "del desc_ridge_preds1f\n",
        "del desc_ridge_preds2\n",
        "del desc_ridge_preds2f\n",
        "gc.collect()\n",
        "print('[{}] Finished garbage collection'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "\n",
        "# 이진화 라벨\n",
        "lb = LabelBinarizer(sparse_output=True)\n",
        "X_brand_train = lb.fit_transform(df_train['brand_name'])\n",
        "X_brand_test = lb.transform(df_test['brand_name'])\n",
        "print('[{}] Finished label binarize `brand_name`'.format(time.time() - start_time))\n",
        "\n",
        "X_cat_train = lb.fit_transform(df_train['gencat_name'])\n",
        "X_cat_test = lb.transform(df_test['gencat_name'])\n",
        "X_cat1_train = lb.fit_transform(df_train['subcat1_name'])\n",
        "X_cat1_test = lb.transform(df_test['subcat1_name'])\n",
        "X_cat2_train = lb.fit_transform(df_train['subcat2_name'])\n",
        "X_cat2_test = lb.transform(df_test['subcat2_name'])\n",
        "print('[{}] Finished label binarize categories'.format(time.time() - start_time))\n",
        "# 압축희소행렬로 변환 csr_matrix\n",
        "X_dummies_train = csr_matrix(\n",
        "    pd.get_dummies(df_train[list(cols - (basic_cols - {'item_condition_id', 'shipping'}))],\n",
        "                  sparse=True).values)\n",
        "print('[{}] Create dummies completed - train'.format(time.time() - start_time))\n",
        "\n",
        "X_dummies_test = csr_matrix(\n",
        "    pd.get_dummies(df_test[list(cols - (basic_cols - {'item_condition_id', 'shipping'}))],\n",
        "                  sparse=True).values)\n",
        "print('[{}] Create dummies completed - test'.format(time.time() - start_time))\n",
        "\n",
        "sparse_merge_train = hstack((X_dummies_train, X_description_train, X_brand_train, X_cat_train,\n",
        "                            X_cat1_train, X_cat2_train, X_name_train)).tocsr()\n",
        "del X_description_train, lb, X_name_train, X_dummies_train\n",
        "gc.collect()\n",
        "print('[{}] Create sparse merge train completed'.format(time.time() - start_time))\n",
        "\n",
        "sparse_merge_test = hstack((X_dummies_test, X_description_test, X_brand_test, X_cat_test,\n",
        "                            X_cat1_test, X_cat2_test, X_name_test)).tocsr()\n",
        "del X_description_test, X_name_test, X_dummies_test\n",
        "gc.collect()\n",
        "print('[{}] Create sparse merge test completed'.format(time.time() - start_time))\n",
        "\n",
        "# 만약 제출 모드면 iter=3\n",
        "if SUBMIT_MODE:\n",
        "    iters = 3\n",
        "else:\n",
        "    iters = 1\n",
        "    rounds = 3\n",
        "\n",
        "# Factorization Machines 추천 모델\n",
        "\n",
        "model = FM_FTRL(alpha=0.035, beta=0.001, L1=0.00001, L2=0.15, D=sparse_merge_train.shape[1],\n",
        "                alpha_fm=0.05, L2_fm=0.0, init_fm=0.01,\n",
        "                D_fm=100, e_noise=0, iters=iters, inv_link=\"identity\", threads=4)\n",
        "\n",
        "if SUBMIT_MODE:\n",
        "    model.fit(sparse_merge_train, y_train)\n",
        "    print('[{}] Train FM completed'.format(time.time() - start_time))\n",
        "    predsFM = model.predict(sparse_merge_test)\n",
        "    print('[{}] Predict FM completed'.format(time.time() - start_time))\n",
        "else:\n",
        "    for i in range(rounds):\n",
        "        model.fit(sparse_merge_train, y_train)\n",
        "        predsFM = model.predict(sparse_merge_test)\n",
        "        print('[{}] Iteration {}/{} -- RMSLE: {}'.format(time.time() - start_time, i + 1, rounds, rmse(predsFM, y_test)))\n",
        "\n",
        "del model\n",
        "gc.collect()\n",
        "if not SUBMIT_MODE:\n",
        "    print(\"FM_FTRL dev RMSLE:\", rmse(predsFM, y_test))\n",
        "\n",
        "\n",
        "fselect = SelectKBest(f_regression, k=48000)\n",
        "train_features = fselect.fit_transform(sparse_merge_train, y_train)\n",
        "test_features = fselect.transform(sparse_merge_test)\n",
        "print('[{}] Select best completed'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "del sparse_merge_train\n",
        "del sparse_merge_test\n",
        "gc.collect()\n",
        "print('[{}] Garbage collection'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "#TFidf 적용\n",
        "tv = TfidfVectorizer(max_features=250000,\n",
        "                    ngram_range=(1, 3),\n",
        "                    stop_words=None)\n",
        "X_name_train = tv.fit_transform(df_train['name'])\n",
        "print('[{}] Finished TFIDF vectorize `name` (1/2)'.format(time.time() - start_time))\n",
        "X_name_test = tv.transform(df_test['name'])\n",
        "print('[{}] Finished TFIDF vectorize `name` (2/2)'.format(time.time() - start_time))\n",
        "\n",
        "tv = TfidfVectorizer(max_features=500000,\n",
        "                    ngram_range=(1, 3),\n",
        "                    stop_words=None)\n",
        "X_description_train = tv.fit_transform(df_train['item_description'])\n",
        "print('[{}] Finished TFIDF vectorize `item_description` (1/2)'.format(time.time() - start_time))\n",
        "X_description_test = tv.transform(df_test['item_description'])\n",
        "print('[{}] Finished TFIDF vectorize `item_description` (2/2)'.format(time.time() - start_time))\n",
        "#압축 희소 행렬 만들기\n",
        "X_dummies_train = csr_matrix(\n",
        "    pd.get_dummies(df_train[['item_condition_id', 'shipping']], sparse=True).values)\n",
        "X_dummies_test = csr_matrix(\n",
        "    pd.get_dummies(df_test[['item_condition_id', 'shipping']], sparse=True).values)\n",
        "\n",
        "sparse_merge_train = hstack((X_description_train, X_brand_train, X_cat_train,\n",
        "                            X_cat1_train, X_cat2_train, X_name_train)).tocsr()\n",
        "del X_dummies_train, X_description_train, X_brand_train, X_cat_train\n",
        "del X_cat1_train, X_cat2_train, X_name_train\n",
        "gc.collect()\n",
        "print('[{}] Create sparse merge train completed'.format(time.time() - start_time))\n",
        "\n",
        "sparse_merge_test = hstack((X_description_test, X_brand_test, X_cat_test,\n",
        "                            X_cat1_test, X_cat2_test, X_name_test)).tocsr()\n",
        "del X_dummies_test, X_description_test, X_brand_test, X_cat_test\n",
        "del X_cat1_test, X_cat2_test, X_name_test\n",
        "gc.collect()\n",
        "print('[{}] Create sparse merge test completed'.format(time.time() - start_time))\n",
        "\n",
        "# 합친 데이터를 분리 후 ridge\n",
        "X_train_1, X_train_2, y_train_1, y_train_2 = train_test_split(sparse_merge_train, y_train,\n",
        "                                                              test_size = 0.5,\n",
        "                                                              shuffle = False)\n",
        "print('[{}] Finished splitting'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "model = Ridge(solver=\"sag\", fit_intercept=True, random_state=205, alpha=3.3)\n",
        "model.fit(X_train_1, y_train_1)\n",
        "print('[{}] Finished to train ridge (1)'.format(time.time() - start_time))\n",
        "ridge_preds1 = model.predict(X_train_2)\n",
        "ridge_preds1f = model.predict(sparse_merge_test)\n",
        "print('[{}] Finished to predict ridge (1)'.format(time.time() - start_time))\n",
        "model = Ridge(solver=\"sag\", fit_intercept=True, random_state=205, alpha=3.3)\n",
        "model.fit(X_train_2, y_train_2)\n",
        "print('[{}] Finished to train ridge (2)'.format(time.time() - start_time))\n",
        "ridge_preds2 = model.predict(X_train_1)\n",
        "ridge_preds2f = model.predict(sparse_merge_test)\n",
        "print('[{}] Finished to predict ridge (2)'.format(time.time() - start_time))\n",
        "ridge_preds_oof = np.concatenate((ridge_preds2, ridge_preds1), axis=0)\n",
        "ridge_preds_test = (ridge_preds1f + ridge_preds2f) / 2.0\n",
        "print('RMSLE OOF: {}'.format(rmse(ridge_preds_oof, y_train)))\n",
        "if not SUBMIT_MODE:\n",
        "    print('RMSLE TEST: {}'.format(rmse(ridge_preds_test, y_test)))\n",
        "\n",
        "# 멀리 나이브 베이즈 사용\n",
        "model = MultinomialNB(alpha=0.01)\n",
        "model.fit(X_train_1, y_train_1 >= 4)\n",
        "print('[{}] Finished to train MNB (1)'.format(time.time() - start_time))\n",
        "mnb_preds1 = model.predict_proba(X_train_2)[:, 1]\n",
        "mnb_preds1f = model.predict_proba(sparse_merge_test)[:, 1]\n",
        "print('[{}] Finished to predict MNB (1)'.format(time.time() - start_time))\n",
        "model = MultinomialNB(alpha=0.01)\n",
        "model.fit(X_train_2, y_train_2 >= 4)\n",
        "print('[{}] Finished to train MNB (2)'.format(time.time() - start_time))\n",
        "mnb_preds2 = model.predict_proba(X_train_1)[:, 1]\n",
        "mnb_preds2f = model.predict_proba(sparse_merge_test)[:, 1]\n",
        "print('[{}] Finished to predict MNB (2)'.format(time.time() - start_time))\n",
        "mnb_preds_oof = np.concatenate((mnb_preds2, mnb_preds1), axis=0)\n",
        "mnb_preds_test = (mnb_preds1f + mnb_preds2f) / 2.0\n",
        "\n",
        "\n",
        "del ridge_preds1\n",
        "del ridge_preds1f\n",
        "del ridge_preds2\n",
        "del ridge_preds2f\n",
        "del mnb_preds1\n",
        "del mnb_preds1f\n",
        "del mnb_preds2\n",
        "del mnb_preds2f\n",
        "del X_train_1\n",
        "del X_train_2\n",
        "del y_train_1\n",
        "del y_train_2\n",
        "del sparse_merge_train\n",
        "del sparse_merge_test\n",
        "del model\n",
        "gc.collect()\n",
        "print('[{}] Finished garbage collection'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "\n",
        "# 릿지 결과를 넣음\n",
        "df_train['ridge'] = ridge_preds_oof\n",
        "df_train['name_ridge'] = name_ridge_preds_oof\n",
        "df_train['desc_ridge'] = desc_ridge_preds_oof\n",
        "df_train['mnb'] = mnb_preds_oof\n",
        "df_test['ridge'] = ridge_preds_test\n",
        "df_test['name_ridge'] = name_ridge_preds_test\n",
        "df_test['desc_ridge'] = desc_ridge_preds_test\n",
        "df_test['mnb'] = mnb_preds_test\n",
        "print('[{}] Finished adding submodels'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "f_cats = ['brand_name', 'gencat_name', 'subcat1_name', 'subcat2_name', 'name_first']\n",
        "target_encode = TargetEncoder(min_samples_leaf=100, smoothing=10, noise_level=0.01,\n",
        "                              keep_original=True, cols=f_cats)\n",
        "df_train, df_test = target_encode.encode(df_train, df_test, y_train)\n",
        "print('[{}] Finished target encoding'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "df_train.drop(f_cats, axis=1, inplace=True)\n",
        "df_test.drop(f_cats, axis=1, inplace=True)\n",
        "del mnb_preds_oof\n",
        "del mnb_preds_test\n",
        "del ridge_preds_oof\n",
        "del ridge_preds_test\n",
        "gc.collect()\n",
        "print('[{}] Finished garbage collection'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "cols = ['gencat_name_te', 'brand_name_te', 'subcat1_name_te', 'subcat2_name_te',\n",
        "        'name_first_te', 'mnb', 'desc_ridge', 'name_ridge', 'ridge']\n",
        "# cols로 희소 압축 행렬 만들기\n",
        "train_dummies = csr_matrix(df_train[cols].values)\n",
        "print('[{}] Finished dummyizing model 1/5'.format(time.time() - start_time))\n",
        "test_dummies = csr_matrix(df_test[cols].values)\n",
        "print('[{}] Finished dummyizing model 2/5'.format(time.time() - start_time))\n",
        "del df_train\n",
        "del df_test\n",
        "gc.collect()\n",
        "print('[{}] Finished dummyizing model 3/5'.format(time.time() - start_time))\n",
        "train_features = hstack((train_features, train_dummies)).tocsr()\n",
        "print('[{}] Finished dummyizing model 4/5'.format(time.time() - start_time))\n",
        "test_features = hstack((test_features, test_dummies)).tocsr()\n",
        "print('[{}] Finished dummyizing model 5/5'.format(time.time() - start_time))\n",
        "\n",
        "#lgb 사용\n",
        "d_train = lgb.Dataset(train_features, label=y_train)\n",
        "del train_features; gc.collect()\n",
        "if SUBMIT_MODE:\n",
        "    watchlist = [d_train]\n",
        "else:\n",
        "    d_valid = lgb.Dataset(test_features, label=y_test)\n",
        "    watchlist = [d_train, d_valid]\n",
        "\n",
        "params = {\n",
        "    'learning_rate': 0.15,\n",
        "    'application': 'regression',\n",
        "    'max_depth': 13,\n",
        "    'num_leaves': 400,\n",
        "    'verbosity': -1,\n",
        "    'metric': 'RMSE',\n",
        "    'data_random_seed': 1,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'feature_fraction': 0.6,\n",
        "    'nthread': 4,\n",
        "    'lambda_l1': 10,\n",
        "    'lambda_l2': 10\n",
        "}\n",
        "print('[{}] Finished compiling LGB'.format(time.time() - start_time))\n",
        "\n",
        "modelL = lgb.train(params,\n",
        "                  train_set=d_train,\n",
        "                  num_boost_round=1350,\n",
        "                  valid_sets=watchlist,\n",
        "                  verbose_eval=50)\n",
        "\n",
        "predsL = modelL.predict(test_features)\n",
        "predsL[predsL < 0] = 0\n",
        "\n",
        "if not SUBMIT_MODE:\n",
        "    print(\"LGB RMSLE:\", rmse(predsL, y_test))\n",
        "\n",
        "del d_train\n",
        "del modelL\n",
        "if not SUBMIT_MODE:\n",
        "    del d_valid\n",
        "gc.collect()\n",
        "\n",
        "# FM 모델의 결과값 가중치 + lgb 가중치 더함\n",
        "preds_final = predsFM * 0.33 + predsL * 0.67\n",
        "if not SUBMIT_MODE:\n",
        "    print('Final RMSE: ', rmse(preds_final, y_test))\n",
        "\n",
        "\n",
        "if SUBMIT_MODE:\n",
        "    preds_final = np.expm1(preds_final)\n",
        "    submission['price'] = preds_final\n",
        "    submission.to_csv('lgb_and_fm_separate_train_test.csv', index=False)\n",
        "    print('[{}] Writing submission done'.format(time.time() - start_time))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.002149343490600586] Finished defining stuff\n",
            "[12.812476396560669] Finished load data\n",
            "[12.820225954055786] compiled train/test\n",
            "Train shape:  (1482535, 9)\n",
            "Test shape:  (693359, 8)\n",
            "[13.149222135543823] Removed nonzero price\n",
            "Train shape:  (1481661, 9)\n",
            "Test shape:  (693359, 8)\n",
            "Index(['test_id', 'name', 'item_condition_id', 'category_name', 'brand_name',\n",
            "       'shipping', 'item_description', 'is_train'],\n",
            "      dtype='object')\n",
            "[13.662659406661987] Compiled merge\n",
            "Merge shape:  (2175020, 10)\n",
            "[14.848224401473999] Garbage collection\n",
            "[21.11609148979187] Split categories completed.\n",
            "[21.789878606796265] Handle missing completed.\n",
            "[27.128100156784058] FE 1/37\n",
            "[27.623751640319824] FE 2/37\n",
            "[27.6563982963562] FE 3/37\n",
            "[27.695059299468994] FE 4/37\n",
            "[27.732656002044678] FE 5/37\n",
            "[27.77166485786438] FE 6/37\n",
            "[32.302391052246094] FE 7/37\n",
            "[51.68472504615784] FE 8/37\n",
            "[54.14115881919861] FE 9/37\n",
            "[60.10906147956848] FE 10/37\n",
            "[60.951597452163696] FE 11/37\n",
            "[62.0220582485199] FE 12/37\n",
            "[62.03787565231323] FE 13/37\n",
            "[66.33812284469604] FE 14/37\n",
            "[71.20150899887085] FE 15/37\n",
            "[72.78069043159485] FE 16/37\n",
            "[74.77459168434143] FE 17/37\n",
            "[74.78992938995361] FE 18/37\n",
            "[74.80344367027283] FE 19/37\n",
            "[75.12413597106934] FE 20/37\n",
            "[75.14316654205322] FE 21/37\n",
            "[75.15726089477539] FE 22/37\n",
            "[75.17159175872803] FE 23/37\n",
            "[76.83096718788147] FE 24/37\n",
            "[81.08846402168274] FE 25/37\n",
            "[81.10500264167786] FE 26/37\n",
            "[81.11882042884827] FE 27/37\n",
            "[82.94411373138428] FE 28/37\n",
            "[87.26347398757935] FE 29/37\n",
            "[87.27813959121704] FE 30/37\n",
            "[87.29273700714111] FE 31/37\n",
            "[98.82752919197083] FE 32/37\n",
            "[115.23341727256775] FE 33/37\n",
            "[123.63826775550842] FE 34/37\n",
            "[126.25042653083801] FE 35/37\n",
            "[131.17066097259521] FE 36/37\n",
            "[131.9887399673462] FE 37/37\n",
            "[133.45805025100708] FE Normalized\n",
            "[134.29162526130676] FE Merged\n",
            "[134.52137231826782] Garbage collection\n",
            "[136.26022386550903] Splitting completed.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-92c9122a0257>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;31m# 이게 어떤 모델인지 모르겠다. https://github.com/anttttti/Wordbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m wb = wordbatch.WordBatch(normalize_text, extractor=(WordBag, {\"hash_ngrams\": 2,\n\u001b[0m\u001b[1;32m    370\u001b[0m                                                               \u001b[0;34m\"hash_ngrams_weights\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m                                                               \u001b[0;34m\"hash_size\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m29\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'wordbatch' has no attribute 'WordBatch'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_xiCk4LGmHF"
      },
      "source": [
        "csr_matrix, Target인코더\n",
        "\n",
        "- **csr_matrix** : 기존의 희소행렬의 경우 대부분의 값이 0이라 연산이 오래 걸리고 메모리 낭비가 심하다. 이를 해결하기 위해 행을 고려하여 압축\n",
        "<https://rfriend.tistory.com/551>\n",
        "\n",
        "- **다항 나이브 베이즈**는 주로 텍스트 분류에 사용된다. 텍스트를 벡터화 한 후 사용합니다. \n",
        "\n",
        "- **Target 인코딩**은 변환시키고자 하는 변수를 선택후 ->  범주형 범수를 그룹화 시키고  타깃변수에 대한 총합의 합계를 얻고 -> 그룹화 시키고 타킷에 대한 빈도수를 얻는다. -> 그리고 총합의 합계를 타깃에 대한 빈도수로 나누고 범주값에 업데이터 하는 기법. <br>\n",
        "\n",
        "여기서는  count가 클수록 avg가 덜 고려되도록 noise와 smoothing을 넣어준거 같다.\n",
        "<https://conanmoon.medium.com/%EB%8D%B0%EC%9D%B4%ED%84%B0%EA%B3%BC%ED%95%99-%EC%9C%A0%EB%A7%9D%EC%A3%BC%EC%9D%98-%EB%A7%A4%EC%9D%BC-%EA%B8%80%EC%93%B0%EA%B8%B0-%EC%9D%BC%EA%B3%B1%EB%B2%88%EC%A7%B8-%EC%9D%BC%EC%9A%94%EC%9D%BC-7a40e7de39d4> \n"
      ]
    }
  ]
}