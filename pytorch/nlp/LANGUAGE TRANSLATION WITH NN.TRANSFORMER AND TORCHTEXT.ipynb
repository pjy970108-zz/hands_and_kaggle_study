{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LANGUAGE TRANSLATION WITH NN.TRANSFORMER AND TORCHTEXT.ipynb","provenance":[{"file_id":"1sDpieIetN5iqCu-JZeVRTk5mjLivVDlN","timestamp":1636887907406},{"file_id":"1w3js7RjSAlBx1PvHhG1xoETJnDOXpPsc","timestamp":1636882902715},{"file_id":"1P7LY7f2pjtKSwayDyQyYKRdCsjPL-JLx","timestamp":1636880272859},{"file_id":"1CjiYnC0L8H-VKTEFtsYebSyVnpPFHnuJ","timestamp":1636878533194},{"file_id":"1ztpS3XkQI_vwKT8eCuxY8ie26DeNfBMr","timestamp":1636875452816}],"collapsed_sections":[],"authorship_tag":"ABX9TyNzo+eTtYpsCOg+f7uXWtIR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Z_bdICdVDaQ6"},"source":["# transformer를 이용하여 번역 모델"]},{"cell_type":"markdown","metadata":{"id":"8QKJb48lY6NG"},"source":["## DATA 준비"]},{"cell_type":"code","metadata":{"id":"FvBoduY6JfrQ","executionInfo":{"status":"ok","timestamp":1636888447503,"user_tz":-540,"elapsed":442,"user":{"displayName":"부계정박준영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13767636744278294407"}}},"source":["# !pip install -U spacy\n","# !python -m spacy download en_core_web_sm\n","# !python -m spacy download de_core_news_sm"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J02ajbkXJJVV","executionInfo":{"status":"ok","timestamp":1636888460956,"user_tz":-540,"elapsed":12903,"user":{"displayName":"부계정박준영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13767636744278294407"}},"outputId":"fbf3b7a2-e469-4057-f8c4-088ad99ff0c5"},"source":["from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","from torchtext.datasets import Multi30k\n","from typing import Iterable, List\n","\n","\n","SRC_LANGUAGE = 'de'\n","TGT_LANGUAGE = 'en'\n","\n","token_transform = {}\n","vocab_transform = {}\n","\n","\n","token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n","token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n","\n","\n","def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n","    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n","\n","    for data_sample in data_iter:\n","        yield token_transform[language](data_sample[language_index[language]])\n","\n","UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n","special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n","\n","for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n","\n","    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n","\n","    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n","                                                    min_freq=1,\n","                                                    specials=special_symbols,\n","                                                    special_first=True)\n","\n","for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n","  vocab_transform[ln].set_default_index(UNK_IDX)"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1.21M/1.21M [00:00<00:00, 1.62MB/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"hs3VZxk5J_oz"},"source":["## seq2seq using Transformer"]},{"cell_type":"code","metadata":{"id":"ekNqIERpJVK_","executionInfo":{"status":"ok","timestamp":1636888620685,"user_tz":-540,"elapsed":328,"user":{"displayName":"부계정박준영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13767636744278294407"}}},"source":["from torch import Tensor\n","import torch\n","import torch.nn as nn\n","from torch.nn import Transformer\n","import math\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# 단어의 위치\n","class PositionalEncoding(nn.Module):\n","    def __init__(self,\n","                 emb_size: int,\n","                 dropout: float,\n","                 maxlen: int = 5000):\n","        super(PositionalEncoding, self).__init__()\n","        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n","        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n","        pos_embedding = torch.zeros((maxlen, emb_size))\n","        pos_embedding[:, 0::2] = torch.sin(pos * den)\n","        pos_embedding[:, 1::2] = torch.cos(pos * den)\n","        pos_embedding = pos_embedding.unsqueeze(-2)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer('pos_embedding', pos_embedding)\n","\n","    def forward(self, token_embedding: Tensor):\n","        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n","\n","# 텐서를 임베딩으로 바꿔줌\n","class TokenEmbedding(nn.Module):\n","    def __init__(self, vocab_size: int, emb_size):\n","        super(TokenEmbedding, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, emb_size)\n","        self.emb_size = emb_size\n","\n","    def forward(self, tokens: Tensor):\n","        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n","\n","# Seq2Seq Network\n","class Seq2SeqTransformer(nn.Module):\n","    def __init__(self,\n","                 num_encoder_layers: int,\n","                 num_decoder_layers: int,\n","                 emb_size: int,\n","                 nhead: int,\n","                 src_vocab_size: int,\n","                 tgt_vocab_size: int,\n","                 dim_feedforward: int = 512,\n","                 dropout: float = 0.1):\n","        super(Seq2SeqTransformer, self).__init__()\n","        self.transformer = Transformer(d_model=emb_size,\n","                                       nhead=nhead,\n","                                       num_encoder_layers=num_encoder_layers,\n","                                       num_decoder_layers=num_decoder_layers,\n","                                       dim_feedforward=dim_feedforward,\n","                                       dropout=dropout)\n","        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n","        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size) #토큰을 임베딩으로\n","        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size) # 타킷 토큰을 임베딩으로\n","        self.positional_encoding = PositionalEncoding( #단어 위치\n","            emb_size, dropout=dropout)\n","\n","    def forward(self,\n","                src: Tensor,\n","                trg: Tensor,\n","                src_mask: Tensor,\n","                tgt_mask: Tensor,\n","                src_padding_mask: Tensor,\n","                tgt_padding_mask: Tensor,\n","                memory_key_padding_mask: Tensor):\n","        src_emb = self.positional_encoding(self.src_tok_emb(src))\n","        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n","        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n","                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n","        return self.generator(outs)\n","\n","    def encode(self, src: Tensor, src_mask: Tensor):\n","        return self.transformer.encoder(self.positional_encoding(\n","                            self.src_tok_emb(src)), src_mask)\n","\n","    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n","        return self.transformer.decoder(self.positional_encoding(\n","                          self.tgt_tok_emb(tgt)), memory,\n","                          tgt_mask)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"DRZ0Vao9KlTo","executionInfo":{"status":"ok","timestamp":1636888660342,"user_tz":-540,"elapsed":329,"user":{"displayName":"부계정박준영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13767636744278294407"}}},"source":["# padding함\n","def generate_square_subsequent_mask(sz):\n","    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n","    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","    return mask\n","\n","\n","def create_mask(src, tgt):\n","    src_seq_len = src.shape[0]\n","    tgt_seq_len = tgt.shape[0]\n","\n","    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n","    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n","\n","    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n","    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n","    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"D-DkpR-zKlJQ","executionInfo":{"status":"ok","timestamp":1636888674797,"user_tz":-540,"elapsed":883,"user":{"displayName":"부계정박준영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13767636744278294407"}}},"source":["torch.manual_seed(0)\n","\n","SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n","TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n","EMB_SIZE = 512\n","NHEAD = 8\n","FFN_HID_DIM = 512\n","BATCH_SIZE = 128\n","NUM_ENCODER_LAYERS = 3\n","NUM_DECODER_LAYERS = 3\n","\n","transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n","                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n","\n","for p in transformer.parameters():\n","    if p.dim() > 1:\n","        nn.init.xavier_uniform_(p)\n","\n","transformer = transformer.to(DEVICE)\n","\n","loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n","\n","optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"fxDk9egoKyZ5","executionInfo":{"status":"ok","timestamp":1636888789298,"user_tz":-540,"elapsed":297,"user":{"displayName":"부계정박준영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13767636744278294407"}}},"source":["from torch.nn.utils.rnn import pad_sequence\n","\n","\n","def sequential_transforms(*transforms):\n","    def func(txt_input):\n","        for transform in transforms:\n","            txt_input = transform(txt_input)\n","        return txt_input\n","    return func\n","\n","# 텐서에 토큰을 추가\n","def tensor_transform(token_ids: List[int]):\n","    return torch.cat((torch.tensor([BOS_IDX]),\n","                      torch.tensor(token_ids),\n","                      torch.tensor([EOS_IDX])))\n","# 사용 데이터와 타깃 단어의 raw string을 tensor로 바꿈\n","text_transform = {}\n","for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n","    text_transform[ln] = sequential_transforms(token_transform[ln], #토크나이즈\n","                                               vocab_transform[ln], #수치화\n","                                               tensor_transform) # BOS/EOS 텐서 만듬\n","\n","\n","# batch\n","def collate_fn(batch):\n","    src_batch, tgt_batch = [], []\n","    for src_sample, tgt_sample in batch:\n","        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n","        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n","\n","    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n","    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n","    return src_batch, tgt_batch"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"VcD-gTZ9KyWb","executionInfo":{"status":"ok","timestamp":1636888819544,"user_tz":-540,"elapsed":289,"user":{"displayName":"부계정박준영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13767636744278294407"}}},"source":["from torch.utils.data import DataLoader\n","\n","\n","# 학습\n","def train_epoch(model, optimizer):\n","    model.train()\n","    losses = 0\n","    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n","    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n","\n","    for src, tgt in train_dataloader:\n","        src = src.to(DEVICE)\n","        tgt = tgt.to(DEVICE)\n","\n","        tgt_input = tgt[:-1, :]\n","\n","        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n","\n","        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n","\n","        optimizer.zero_grad()\n","\n","        tgt_out = tgt[1:, :]\n","        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n","        loss.backward()\n","\n","        optimizer.step()\n","        losses += loss.item()\n","\n","    return losses / len(train_dataloader)\n","\n","# 평가\n","def evaluate(model):\n","    model.eval()\n","    losses = 0\n","\n","    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n","    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n","\n","    for src, tgt in val_dataloader:\n","        src = src.to(DEVICE)\n","        tgt = tgt.to(DEVICE)\n","\n","        tgt_input = tgt[:-1, :]\n","\n","        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n","\n","        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n","\n","        tgt_out = tgt[1:, :]\n","        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n","        losses += loss.item()\n","\n","    return losses / len(val_dataloader)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l7zHvkfPLTRz","outputId":"d8c3c6bb-2311-4c9c-f06e-3ba04f19731f"},"source":["from timeit import default_timer as timer\n","NUM_EPOCHS = 18\n","\n","for epoch in range(1, NUM_EPOCHS+1):\n","    start_time = timer()\n","    train_loss = train_epoch(transformer, optimizer)\n","    end_time = timer()\n","    val_loss = evaluate(transformer)\n","    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n","\n","# greedy를 통해 output 생성\n","def greedy_decode(model, src, src_mask, max_len, start_symbol):\n","    src = src.to(DEVICE)\n","    src_mask = src_mask.to(DEVICE)\n","\n","    memory = model.encode(src, src_mask)\n","    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n","    for i in range(max_len-1):\n","        memory = memory.to(DEVICE)\n","        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n","                    .type(torch.bool)).to(DEVICE)\n","        out = model.decode(ys, memory, tgt_mask)\n","        out = out.transpose(0, 1)\n","        prob = model.generator(out[:, -1])\n","        _, next_word = torch.max(prob, dim=1) # 확률이 높은 것을 선택\n","        next_word = next_word.item()\n","\n","        ys = torch.cat([ys,\n","                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n","        if next_word == EOS_IDX:\n","            break\n","    return ys\n","\n","\n","# 실제 비교\n","def translate(model: torch.nn.Module, src_sentence: str):\n","    model.eval()\n","    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n","    num_tokens = src.shape[0]\n","    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n","    tgt_tokens = greedy_decode(\n","        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n","    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 46.3k/46.3k [00:00<00:00, 282kB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 1, Train loss: 5.342, Val loss: 4.138, Epoch time = 1942.370s\n"]}]},{"cell_type":"code","metadata":{"id":"v73_1oMcLlqt"},"source":["print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"],"execution_count":null,"outputs":[]}]}