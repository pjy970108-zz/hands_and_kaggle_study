{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TRAIN A MARIO-PLAYING RL AGENT.ipynb","provenance":[{"file_id":"1LqxIG-ah6ru2HSVMBBI6JUtNyWbd1K_E","timestamp":1638686512313}],"collapsed_sections":[],"authorship_tag":"ABX9TyOzKXE694dVJ+cJSCo5uF9S"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ctnSoXekVEEH"},"source":["# !pip install gym-super-mario-bros==7.3.0\n","\n","import torch\n","from torch import nn\n","from torchvision import transforms as T\n","from PIL import Image\n","import numpy as np\n","from pathlib import Path\n","from collections import deque\n","import random, datetime, os, copy\n","\n","# Gym is an OpenAI toolkit for RL\n","import gym\n","from gym.spaces import Box\n","from gym.wrappers import FrameStack\n","\n","# NES Emulator for OpenAI Gym\n","from nes_py.wrappers import JoypadSpace\n","\n","# Super Mario environment for OpenAI Gym\n","import gym_super_mario_bros"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NP34TYEoVck8"},"source":["- Enviroment : 마리오 세계\n","- action : 환경에서 어떻게 agent가 반응하는지, state-action\n","- state : 환경의 현재 특징. state-space\n","- reward : 환경에서 agent에게 주는 보상. agent는 미래 행위 action을 배운다.\n","- optimal action-value function : state S 시작하면 action을 취한다. 그 뒤에 Q를 최대화하는 행위를 택함"]},{"cell_type":"markdown","metadata":{"id":"OhkVG-ajWjYk"},"source":["## Enviroment"]},{"cell_type":"code","metadata":{"id":"Z7KVt4m9Wk9S"},"source":["env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\")\n","\n","# 행위 공간 제한\n","#   0. 오른쪽으로 걷는다\n","#   1. 오른쪽으로 점프\n","env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n","\n","env.reset()\n","next_state, reward, done, info = env.step(action=0)\n","print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MtQKe9ppW2cK"},"source":["## Environment 전처리\n","\n","환경데이터가 next_state로 반환된다. 환경데이터는 [3, 240, 256]으로 우리가 필요한 것보다 많은 data를 갖고 있다. 마리오 행동은 색깔에 영향을 안받기 때문\n","<br>\n","\n","- 회색으로 바꾸고, resize해준다."]},{"cell_type":"code","metadata":{"id":"XhzFJConWlAL"},"source":["class SkipFrame(gym.Wrapper):\n","    def __init__(self, env, skip):\n","        \"\"\"Return only every `skip`-th frame\"\"\"\n","        super().__init__(env)\n","        self._skip = skip\n","\n","    def step(self, action):\n","        \"\"\"행동을 반복하고 reward를 합함\"\"\"\n","        total_reward = 0.0\n","        done = False\n","        for i in range(self._skip):\n","            # 같은 행동과 보상의 축적\n","            obs, reward, done, info = self.env.step(action)\n","            total_reward += reward\n","            if done:\n","                break\n","        return obs, total_reward, done, info\n","\n","\n","class GrayScaleObservation(gym.ObservationWrapper):\n","    def __init__(self, env):\n","        super().__init__(env)\n","        obs_shape = self.observation_space.shape[:2]\n","        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n","\n","    def permute_orientation(self, observation):\n","        # [H, W, C]를 [C, H, W]로 바꿈\n","        observation = np.transpose(observation, (2, 0, 1))\n","        observation = torch.tensor(observation.copy(), dtype=torch.float)\n","        return observation\n","\n","    def observation(self, observation):\n","        observation = self.permute_orientation(observation)\n","        transform = T.Grayscale()\n","        observation = transform(observation)\n","        return observation\n","\n","\n","class ResizeObservation(gym.ObservationWrapper):\n","    def __init__(self, env, shape):\n","        super().__init__(env)\n","        if isinstance(shape, int):\n","            self.shape = (shape, shape)\n","        else:\n","            self.shape = tuple(shape)\n","\n","        obs_shape = self.shape + self.observation_space.shape[2:]\n","        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n","\n","    def observation(self, observation):\n","        transforms = T.Compose(\n","            [T.Resize(self.shape), T.Normalize(0, 255)]\n","        )\n","        observation = transforms(observation).squeeze(0)\n","        return observation\n","\n","\n","# Apply Wrappers to environment\n","env = SkipFrame(env, skip=4)\n","env = GrayScaleObservation(env)\n","env = ResizeObservation(env, shape=84)\n","env = FrameStack(env, num_stack=4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jPJ7mXPSYuLn"},"source":["## agent\n","\n","- act: 현재 상태에서 최적의 행위\n","- remember : 경험(현재 상태, 현재 행동, 보상, 다음 상태)\n","- learn : 더 나은 행위를 배움"]},{"cell_type":"code","metadata":{"id":"zXlqSGVPYbpi"},"source":["class Mario:\n","    def __init__():\n","        pass\n","\n","    def act(self, state):\n","       \"\"\" 행위가 주어졌을때, epsilon-greedy 행위 선택\"\"\"\n","        pass\n","\n","    def cache(self, experience):\n","        \"\"\"메모리에 경험 저장\"\"\"\n","        pass\n","\n","    def recall(self):\n","        \"\"\"메모리로 부터 샘플 경험\"\"\"\n","        pass\n","\n","    def learn(self):\n","        \"\"\"행동 업데이트\"\"\"\n","        pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vrUTsfDbZX13"},"source":["## ACT\n","\n","상태가 주어지면, 가장 최적 행동 또는 random 행동 선택"]},{"cell_type":"code","metadata":{"id":"ocFMuPh0ZWFm"},"source":["class Mario:\n","    def __init__(self, state_dim, action_dim, save_dir):\n","        self.state_dim = state_dim\n","        self.action_dim = action_dim\n","        self.save_dir = save_dir\n","\n","        self.use_cuda = torch.cuda.is_available()\n","\n","        # 마리오 DNN 최적 행동 예측 -> learn 단계에서 할거\n","        self.net = MarioNet(self.state_dim, self.action_dim).float()\n","        if self.use_cuda:\n","            self.net = self.net.to(device=\"cuda\")\n","\n","        self.exploration_rate = 1\n","        self.exploration_rate_decay = 0.99999975\n","        self.exploration_rate_min = 0.1\n","        self.curr_step = 0\n","\n","        self.save_every = 5e5 \n","\n","    def act(self, state):\n","        \"\"\"\n","    상태가 주어지고 action과 스텝의 가치를 업데이트\n","    Inputs:\n","    state(LazyFrame): state_dim을 차원으로 현재 상태에 대한 단일 관찰\n","    Outputs:\n","    action_idx (int): 마리오가 행동했을때 정수\n","    \"\"\"\n","        # EXPLORE\n","        if np.random.rand() < self.exploration_rate:\n","            action_idx = np.random.randint(self.action_dim)\n","\n","        # EXPLOIT\n","        else:\n","            state = state.__array__()\n","            if self.use_cuda:\n","                state = torch.tensor(state).cuda()\n","            else:\n","                state = torch.tensor(state)\n","            state = state.unsqueeze(0)\n","            action_values = self.net(state, model=\"online\")\n","            action_idx = torch.argmax(action_values, axis=1).item()\n","\n","        # exploration rate 감소\n","        self.exploration_rate *= self.exploration_rate_decay\n","        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n","\n","        # 증가\n","        self.curr_step += 1\n","        return action_idx"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ghguw8zjZ_m_"},"source":["## cache and recall\n","\n","- cache : 저장\n","- recall : 사용"]},{"cell_type":"code","metadata":{"id":"CLhu7j0uZWC4"},"source":["class Mario(Mario):  # subclassing for continuity\n","    def __init__(self, state_dim, action_dim, save_dir):\n","        super().__init__(state_dim, action_dim, save_dir)\n","        self.memory = deque(maxlen=100000)\n","        self.batch_size = 32\n","\n","    def cache(self, state, next_state, action, reward, done):\n","        \"\"\"\n","        Store the experience to self.memory (replay buffer)\n","\n","        Inputs:\n","        state (LazyFrame),\n","        next_state (LazyFrame),\n","        action (int),\n","        reward (float),\n","        done(bool))\n","        \"\"\"\n","        state = state.__array__()\n","        next_state = next_state.__array__()\n","\n","        if self.use_cuda:\n","            state = torch.tensor(state).cuda()\n","            next_state = torch.tensor(next_state).cuda()\n","            action = torch.tensor([action]).cuda()\n","            reward = torch.tensor([reward]).cuda()\n","            done = torch.tensor([done]).cuda()\n","        else:\n","            state = torch.tensor(state)\n","            next_state = torch.tensor(next_state)\n","            action = torch.tensor([action])\n","            reward = torch.tensor([reward])\n","            done = torch.tensor([done])\n","\n","        self.memory.append((state, next_state, action, reward, done,))\n","\n","    def recall(self):\n","        \"\"\"\n","        Retrieve a batch of experiences from memory\n","        \"\"\"\n","        batch = random.sample(self.memory, self.batch_size)\n","        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n","        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sr_YujbrbD51"},"source":["# Learn\n","\n","- DDQN 사용 : 2개의 conv 사용(온라인, target)\n","\n","- Q(온라인)과 Q(target)의 공유하는 특징 생성기 만듬. 그러나 분리된 FC Classifier을 유지하기 위해 parameter(target)은 역전파 과정에서 고정. 대신 parameter(online)은 학습"]},{"cell_type":"code","metadata":{"id":"fNkmrIaBbDTM"},"source":["class MarioNet(nn.Module):\n","    \"\"\"mini cnn structure\n","  input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n","  \"\"\"\n","\n","    def __init__(self, input_dim, output_dim):\n","        super().__init__()\n","        c, h, w = input_dim\n","\n","        if h != 84:\n","            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n","        if w != 84:\n","            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n","\n","        self.online = nn.Sequential(\n","            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n","            nn.ReLU(),\n","            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n","            nn.ReLU(),\n","            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(3136, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, output_dim),\n","        )\n","\n","        self.target = copy.deepcopy(self.online)\n","\n","        # Q_target parameters 고정.\n","        for p in self.target.parameters():\n","            p.requires_grad = False\n","\n","    def forward(self, input, model):\n","        if model == \"online\":\n","            return self.online(input)\n","        elif model == \"target\":\n","            return self.target(input)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4eUHI08Zbqh5"},"source":["## TD estimate & TD Target\n","\n","- TD estimate : 최적의 Q 예측\n","- TD target :다음 State에서 현재 보상과 Q예측\n","\n","<br>\n","우리는 다음 action을 모르기에 우리는 Q(online)을 최대화하는 action을 선택함"]},{"cell_type":"code","metadata":{"id":"MRPYJEH_bqXp"},"source":["class Mario(Mario):\n","    def __init__(self, state_dim, action_dim, save_dir):\n","        super().__init__(state_dim, action_dim, save_dir)\n","        self.gamma = 0.9\n","\n","    def td_estimate(self, state, action):\n","        current_Q = self.net(state, model=\"online\")[\n","            np.arange(0, self.batch_size), action\n","        ]  # Q_online(s,a)\n","        return current_Q\n","\n","    @torch.no_grad() # bacpropa 안하기 위해 사용)\n","    def td_target(self, reward, next_state, done):\n","        next_state_Q = self.net(next_state, model=\"online\")\n","        best_action = torch.argmax(next_state_Q, axis=1)\n","        next_Q = self.net(next_state, model=\"target\")[\n","            np.arange(0, self.batch_size), best_action\n","        ]\n","        return (reward + (1 - done.float()) * self.gamma * next_Q).float()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N2dF4Sgvcbd6"},"source":["## 모델 업데이트\n"]},{"cell_type":"code","metadata":{"id":"NV9DLJWrbqVB"},"source":["class Mario(Mario):\n","    def __init__(self, state_dim, action_dim, save_dir):\n","        super().__init__(state_dim, action_dim, save_dir)\n","        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n","        self.loss_fn = torch.nn.SmoothL1Loss()\n","\n","    def update_Q_online(self, td_estimate, td_target):\n","        loss = self.loss_fn(td_estimate, td_target)\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","        return loss.item()\n","\n","    def sync_Q_target(self):\n","        self.net.target.load_state_dict(self.net.online.state_dict())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SKECdAE6cjHL"},"source":["class Mario(Mario):\n","    def save(self):\n","        save_path = (\n","            self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n","        )\n","        torch.save(\n","            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n","            save_path,\n","        )\n","        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qJp7DjoPcjEB"},"source":["class Mario(Mario):\n","    def __init__(self, state_dim, action_dim, save_dir):\n","        super().__init__(state_dim, action_dim, save_dir)\n","        self.burnin = 1e4  # min. experiences before training\n","        self.learn_every = 3  # no. of experiences between updates to Q_online\n","        self.sync_every = 1e4  # no. of experiences between Q_target & Q_online sync\n","\n","    def learn(self):\n","        if self.curr_step % self.sync_every == 0:\n","            self.sync_Q_target()\n","\n","        if self.curr_step % self.save_every == 0:\n","            self.save()\n","\n","        if self.curr_step < self.burnin:\n","            return None, None\n","\n","        if self.curr_step % self.learn_every != 0:\n","            return None, None\n","\n","        # Sample from memory\n","        state, next_state, action, reward, done = self.recall()\n","\n","        # Get TD Estimate\n","        td_est = self.td_estimate(state, action)\n","\n","        # Get TD Target\n","        td_tgt = self.td_target(reward, next_state, done)\n","\n","        # Backpropagate loss through Q_online\n","        loss = self.update_Q_online(td_est, td_tgt)\n","\n","        return (td_est.mean().item(), loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RR0PciSoc_16"},"source":["import numpy as np\n","import time, datetime\n","import matplotlib.pyplot as plt\n","\n","\n","class MetricLogger:\n","    def __init__(self, save_dir):\n","        self.save_log = save_dir / \"log\"\n","        with open(self.save_log, \"w\") as f:\n","            f.write(\n","                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n","                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n","                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n","            )\n","        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n","        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n","        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n","        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n","\n","        # History metrics\n","        self.ep_rewards = []\n","        self.ep_lengths = []\n","        self.ep_avg_losses = []\n","        self.ep_avg_qs = []\n","\n","        # Moving averages, added for every call to record()\n","        self.moving_avg_ep_rewards = []\n","        self.moving_avg_ep_lengths = []\n","        self.moving_avg_ep_avg_losses = []\n","        self.moving_avg_ep_avg_qs = []\n","\n","        # Current episode metric\n","        self.init_episode()\n","\n","        # Timing\n","        self.record_time = time.time()\n","\n","    def log_step(self, reward, loss, q):\n","        self.curr_ep_reward += reward\n","        self.curr_ep_length += 1\n","        if loss:\n","            self.curr_ep_loss += loss\n","            self.curr_ep_q += q\n","            self.curr_ep_loss_length += 1\n","\n","    def log_episode(self):\n","        \"Mark end of episode\"\n","        self.ep_rewards.append(self.curr_ep_reward)\n","        self.ep_lengths.append(self.curr_ep_length)\n","        if self.curr_ep_loss_length == 0:\n","            ep_avg_loss = 0\n","            ep_avg_q = 0\n","        else:\n","            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n","            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n","        self.ep_avg_losses.append(ep_avg_loss)\n","        self.ep_avg_qs.append(ep_avg_q)\n","\n","        self.init_episode()\n","\n","    def init_episode(self):\n","        self.curr_ep_reward = 0.0\n","        self.curr_ep_length = 0\n","        self.curr_ep_loss = 0.0\n","        self.curr_ep_q = 0.0\n","        self.curr_ep_loss_length = 0\n","\n","    def record(self, episode, epsilon, step):\n","        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n","        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n","        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n","        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n","        self.moving_avg_ep_rewards.append(mean_ep_reward)\n","        self.moving_avg_ep_lengths.append(mean_ep_length)\n","        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n","        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n","\n","        last_record_time = self.record_time\n","        self.record_time = time.time()\n","        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n","\n","        print(\n","            f\"Episode {episode} - \"\n","            f\"Step {step} - \"\n","            f\"Epsilon {epsilon} - \"\n","            f\"Mean Reward {mean_ep_reward} - \"\n","            f\"Mean Length {mean_ep_length} - \"\n","            f\"Mean Loss {mean_ep_loss} - \"\n","            f\"Mean Q Value {mean_ep_q} - \"\n","            f\"Time Delta {time_since_last_record} - \"\n","            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n","        )\n","\n","        with open(self.save_log, \"a\") as f:\n","            f.write(\n","                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n","                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n","                f\"{time_since_last_record:15.3f}\"\n","                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n","            )\n","\n","        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n","            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n","            plt.savefig(getattr(self, f\"{metric}_plot\"))\n","            plt.clf()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eXc9SPjQdASR"},"source":["use_cuda = torch.cuda.is_available()\n","print(f\"Using CUDA: {use_cuda}\")\n","print()\n","\n","save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n","save_dir.mkdir(parents=True)\n","\n","mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n","\n","logger = MetricLogger(save_dir)\n","\n","episodes = 10\n","for e in range(episodes):\n","\n","    state = env.reset()\n","\n","    # Play the game!\n","    while True:\n","\n","        # Run agent on the state\n","        action = mario.act(state)\n","\n","        # Agent performs action\n","        next_state, reward, done, info = env.step(action)\n","\n","        # Remember\n","        mario.cache(state, next_state, action, reward, done)\n","\n","        # Learn\n","        q, loss = mario.learn()\n","\n","        # Logging\n","        logger.log_step(reward, loss, q)\n","\n","        # Update state\n","        state = next_state\n","\n","        # Check if end of game\n","        if done or info[\"flag_get\"]:\n","            break\n","\n","    logger.log_episode()\n","\n","    if e % 20 == 0:\n","        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)\n"],"execution_count":null,"outputs":[]}]}